{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIT406_Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eJ2-CCxYSl6W",
        "Gw76Oty9z2IS",
        "E-_8PaWLuPxs",
        "mBJD0iDTVaRz",
        "U92eGyAx337R",
        "BymmeKqgBJG3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ2-CCxYSl6W"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_VjdDvojiO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw76Oty9z2IS"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwJNT9PTouDh"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEJS5x_voxco"
      },
      "source": [
        "import io \n",
        "\n",
        "#spam\n",
        "spam_df = pd.read_csv('spam_df2.csv')\n",
        "\n",
        "#easy ham\n",
        "easy_ham_df = pd.read_csv('easy_ham_df2.csv', engine = 'python',error_bad_lines= False)\n",
        "\n",
        "#hard ham\n",
        "hard_ham_df = pd.read_csv('hard_ham_df2.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-_8PaWLuPxs"
      },
      "source": [
        "### Preprocessing: Train and test split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDIUX1CPrnU1"
      },
      "source": [
        "#Splitting spam messages into 80% train and 20% test\n",
        "spam_train = spam_df.sample(frac = 0.8,random_state=25)\n",
        "spam_test = spam_df.drop(spam_train.index,axis = 0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCquy5hbw_UK"
      },
      "source": [
        "#function that creates a training and testing split (used to create a balanced dataset between\n",
        "# spam and ham messages)\n",
        "def train_test(df,nr_train,nr_test):\n",
        "  train = df.sample(n = nr_train,random_state=25)\n",
        "  index = train.index\n",
        "  df_update = df.drop(index,axis = 0)\n",
        "  test = df_update.sample(n = nr_test,random_state = 25)\n",
        "\n",
        "  train = train[['text','label']]\n",
        "  test = test[['text','label']]\n",
        "\n",
        "  return train,test\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uUyujoSxCLU"
      },
      "source": [
        "#training and testing for ham(easy) and ham(hard)\n",
        "easy_train, easy_test = train_test(easy_ham_df,len(spam_train),len(spam_test))\n",
        "\n",
        "hard_train = hard_ham_df.sample(frac = 0.8,random_state = 25)\n",
        "hard_test = hard_ham_df.drop(hard_train.index,axis = 0)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBJD0iDTVaRz"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG-9hSTjVc8D"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# training easy\n",
        "X_easy = pd.concat([easy_train,spam_train])\n",
        "X_easy = X_easy.drop('Unnamed: 0',1)\n",
        "\n",
        "#testing easy\n",
        "x_easy = pd.concat([easy_test,spam_test])\n",
        "x_easy = x_easy.drop('Unnamed: 0',1)\n",
        "\n",
        "#training hard\n",
        "X_hard = pd.concat([hard_train,spam_train.sample(n = len(hard_train),random_state= 25)])\n",
        "X_hard = X_hard.drop('Unnamed: 0',1)\n",
        "\n",
        "#testing hard\n",
        "x_hard = pd.concat([hard_test,spam_test.sample(n = len(hard_test),random_state = 25)])\n",
        "x_hard = x_hard.drop('Unnamed: 0',1)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4W_I7-1bPz0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "multi_NB = MultinomialNB()\n",
        "bernoulli_NB = BernoulliNB()\n",
        "\n",
        "# Function that inputs training data, testing data, mtype = {'multinomial','bernoulli'}\n",
        "# dtype = {'easy_ham','hard_ham'}, stopwords = list of words. Function returns a confusion matrix\n",
        "# from classifying on the testing data and the class accuracies\n",
        "def Naive_Bayes(train,test,mtype,dtype,stopwords):\n",
        "\n",
        "  if mtype == \"multinomial\":\n",
        "    count_vec = CountVectorizer(stop_words = stopwords)\n",
        "    \n",
        "    X = count_vec.fit_transform(train.text)\n",
        "    x = count_vec.transform(test.text)\n",
        "    \n",
        "    clf = multi_NB.fit(X,train.label)\n",
        "    pred = clf.predict(x)\n",
        "    matrix = confusion_matrix(test.label,pred,labels = ['spam',dtype])\n",
        "    acc_spam = matrix[0,0]/(matrix[0,0] + matrix[0,1])\n",
        "    acc_ham = matrix[1,1]/(matrix[1,1]+matrix[1,0])\n",
        "    print(\"Result multinomial NB:\",dtype,sep = \" \")\n",
        "    print(matrix)\n",
        "    print(\"Accuracy spam:\",acc_spam,sep = \" \")\n",
        "    print(\"Accuracy ham:\",acc_ham, sep = \" \")\n",
        "    print(\"\\n\")\n",
        "  elif mtype == \"bernoulli\":\n",
        "    count_vec = CountVectorizer(stop_words= stopwords, binary = True)\n",
        "\n",
        "    X = count_vec.fit_transform(train.text)\n",
        "    x = count_vec.transform(test.text)\n",
        "\n",
        "    clf = bernoulli_NB.fit(X,train.label)\n",
        "    pred = clf.predict(x)\n",
        "    matrix = confusion_matrix(test.label,pred,labels = ['spam',dtype])\n",
        "    acc_spam = matrix[0,0]/(matrix[0,0] + matrix[0,1])\n",
        "    acc_ham = matrix[1,1]/(matrix[1,1]+matrix[1,0])\n",
        "    print(\"Result Bernoulli NB:\",dtype,sep = \" \")\n",
        "    print(matrix)\n",
        "    print(\"Accuracy spam:\",acc_spam,sep = \" \")\n",
        "    print(\"Accuracy ham:\",acc_ham, sep = \" \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  \n",
        "  return X\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtYmZM89YAk4"
      },
      "source": [
        "# Running Naive Bayes \n",
        "X_easy_mn = Naive_Bayes(X_easy,x_easy,'multinomial','easy_ham',stopwords= [])\n",
        "X_hard_mn = Naive_Bayes(X_hard,x_hard,'multinomial','hard_ham',stopwords = [])\n",
        "X_easy_bn = Naive_Bayes(X_easy,x_easy,'bernoulli','easy_ham',stopwords = [])\n",
        "X_hard_bn = Naive_Bayes(X_hard,x_hard,'bernoulli','hard_ham',stopwords = [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U92eGyAx337R"
      },
      "source": [
        "### Common Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q4_9nBWES1y"
      },
      "source": [
        "#Function that inputs a dataframe and number of common and uncommon words to generate\n",
        "# returns a concatenated list of common and uncommon words and \n",
        "# separate lists with counts of how many times the words occur in the data.\n",
        "def words_list(X,n_common,n_uncommon):\n",
        "  vect = CountVectorizer()  \n",
        "  X_counts = vect.fit_transform(X.text)\n",
        "  \n",
        "  counts = pd.DataFrame(X_counts.toarray(), columns=vect.get_feature_names_out())\n",
        "  common = counts.sum(axis = 0).sort_values(ascending = False).head(n_common)\n",
        "  uncommon = counts.sum(axis = 0).sort_values(ascending = True).head(n_uncommon)\n",
        "\n",
        "  return_list = []\n",
        "  for i in range(len(common)):\n",
        "    return_list.append(common.index[i])\n",
        "  \n",
        "  for j in range(len(uncommon)):\n",
        "\n",
        "    return_list.append(uncommon.index[j])\n",
        "\n",
        "  return return_list, common, uncommon\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZgD1QbjJD7E"
      },
      "source": [
        "# Common and uncommon words from our three dataset\n",
        "easy_words, easy_common, easy_uncommon = words_list(easy_ham_df,50,0)\n",
        "hard_words, hard_common, hard_uncommon = words_list(hard_ham_df,50,0)\n",
        "spam_words, spam_common, spam_uncommon = words_list(spam_df,50,0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAiVFVO63J6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yng8-mr3N10W"
      },
      "source": [
        "# Words that are common in both easy ham vs spam and hard ham vs spam respectively\n",
        "common_easy = list(set(easy_words).intersection(spam_words))\n",
        "common_hard = list(set(hard_words).intersection(spam_words))\n",
        "# Words  that are uncommon\n",
        "uncommon_easy = [i for i in easy_uncommon.index]\n",
        "uncommon_hard = [i for i in hard_uncommon.index]\n",
        "uncommon_spam = [i for i in spam_uncommon.index]\n",
        "\n",
        "# Final list of stopwords to pass in the functio Naive_Bayes\n",
        "stop_easy = common_easy + uncommon_easy + uncommon_spam\n",
        "stop_hard = common_hard + uncommon_hard + uncommon_spam"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkMqRyzU2O1"
      },
      "source": [
        "# Running Naive Bayes with stopwords\n",
        "Naive_Bayes(X_easy,x_easy,'multinomial','easy_ham' ,stopwords = stop_easy)\n",
        "Naive_Bayes(X_hard,x_hard,'multinomial','hard_ham',stopwords = stop_hard)\n",
        "Naive_Bayes(X_easy,x_easy,'bernoulli','easy_ham',stopwords = stop_easy)\n",
        "Naive_Bayes(X_hard,x_hard,'bernoulli','hard_ham',stopwords = stop_hard)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BymmeKqgBJG3"
      },
      "source": [
        "### Filtering of headers and footer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wISy_rHGEfPs"
      },
      "source": [
        "# Function that filters out any text in data that is not between the first and last indices of\n",
        "# quotation mark (\")\n",
        "def filter_out(data):\n",
        "\n",
        "  df_out = pd.DataFrame()\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    txt = data.text.iloc[i]\n",
        "    start = txt.find('\"')\n",
        "    finish = txt.rfind('\"')\n",
        "    out = txt[start+1:finish]\n",
        "    df_out = df_out.append({'text' : out},ignore_index = True)\n",
        "  \n",
        "  df_out['label'] = [i for i in data.label]\n",
        "  return df_out\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGwu5AMCHMHX"
      },
      "source": [
        "# Filtering training and testing data\n",
        "X_easy_filtered = filter_out(X_easy)\n",
        "X_hard_filtered = filter_out(X_hard)\n",
        "\n",
        "x_easy_filtered = filter_out(x_easy)\n",
        "x_hard_filtered = filter_out(x_hard)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGOMWIrHTXRJ"
      },
      "source": [
        "# Running Naive Bayes on filtered data\n",
        "X_easy_mn = Naive_Bayes(X_easy_filtered,x_easy_filtered,'multinomial','easy_ham',stopwords= [])\n",
        "X_hard_mn = Naive_Bayes(X_hard_filtered,x_hard_filtered,'multinomial','hard_ham',stopwords = [])\n",
        "X_easy_bn  = Naive_Bayes(X_easy_filtered,x_easy_filtered,'bernoulli','easy_ham',stopwords = [])\n",
        "X_hard_bn = Naive_Bayes(X_hard_filtered,x_hard_filtered,'bernoulli','hard_ham',stopwords = [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2NLisq03ZqA"
      },
      "source": [
        "# Running Naive Bayes on filtered data with stop words\n",
        "X_easy_mn = Naive_Bayes(X_easy_filtered,x_easy_filtered,'multinomial','easy_ham',stopwords = stop_easy )\n",
        "X_hard_mn = Naive_Bayes(X_hard_filtered,x_hard_filtered,'multinomial','hard_ham',stopwords = stop_hard)\n",
        "X_easy_bn  = Naive_Bayes(X_easy_filtered,x_easy_filtered,'bernoulli','easy_ham',stopwords = stop_easy)\n",
        "X_hard_bn = Naive_Bayes(X_hard_filtered,x_hard_filtered,'bernoulli','hard_ham',stopwords = stop_hard)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}